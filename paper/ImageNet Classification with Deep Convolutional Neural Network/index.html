<!doctype html>
<html lang="zh-Hans" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-paper">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.3.1">
<title data-rh="true">AlexNet | Brick Bar</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://brickbar1024.github.io/paper/ImageNet Classification with Deep Convolutional Neural Network"><meta data-rh="true" name="docusaurus_locale" content="zh-Hans"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="zh-Hans"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="AlexNet | Brick Bar"><meta data-rh="true" name="description" content="ImageNet Classification with Deep Convolutional Neural Network"><meta data-rh="true" property="og:description" content="ImageNet Classification with Deep Convolutional Neural Network"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2023-05-22T06:42:48.000Z"><meta data-rh="true" property="article:author" content="http://joyceliang.club/"><meta data-rh="true" property="article:tag" content="Deeplearning,Image Classification"><link data-rh="true" rel="icon" href="/img/favicon2.png"><link data-rh="true" rel="canonical" href="https://brickbar1024.github.io/paper/ImageNet Classification with Deep Convolutional Neural Network"><link data-rh="true" rel="alternate" href="https://brickbar1024.github.io/paper/ImageNet Classification with Deep Convolutional Neural Network" hreflang="zh-Hans"><link data-rh="true" rel="alternate" href="https://brickbar1024.github.io/paper/ImageNet Classification with Deep Convolutional Neural Network" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/paper/rss.xml" title="Brick Bar RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/paper/atom.xml" title="Brick Bar Atom Feed">
<link rel="alternate" type="application/rss+xml" href="/hodgwpodge/rss.xml" title="Brick Bar RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/hodgwpodge/atom.xml" title="Brick Bar Atom Feed">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.20/dist/katex.min.css" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.a29f4144.css">
<link rel="preload" href="/assets/js/runtime~main.e24bfab6.js" as="script">
<link rel="preload" href="/assets/js/main.f55d07d9.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}(),document.documentElement.setAttribute("data-announcement-bar-initially-dismissed",function(){try{return"true"===localStorage.getItem("docusaurus.announcement.dismiss")}catch(t){}return!1}())</script><div id="__docusaurus">
<div role="region" aria-label="跳到主要内容"><a class="skipToContent_fXgn" href="#docusaurus_skipToContent_fallback">跳到主要内容</a></div><div class="announcementBar_mb4j" role="banner"><div class="announcementBarPlaceholder_vyr4"></div><div class="content_knG7 announcementBarContent_xLdY">⭐️ If you like BrickBar, give it a star on <a target="_blank" rel="noopener noreferrer" href="https://github.com/BrickBar1024/brickbar1024.github.io">GitHub</a></div><button type="button" aria-label="关闭" class="clean-btn close closeButton_CVFx announcementBarClose_gvF7"><svg viewBox="0 0 15 15" width="14" height="14"><g stroke="currentColor" stroke-width="3.1"><path d="M.75.75l13.5 13.5M14.25.75L.75 14.25"></path></g></svg></button></div><nav aria-label="主导航" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="切换导航栏" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/favicon2.png" alt="My Site Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/favicon2.png" alt="My Site Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">BrickBar</b></a><a class="navbar__item navbar__link" href="/docs/intro">Notes</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/paper">Paper</a><a class="navbar__item navbar__link" href="/hodgwpodge">Hodgwpodge</a><a class="navbar__item navbar__link" href="/showcase">News</a></div><div class="navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/about">Team</a><a href="https://github.com/BrickBar1024" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="切换浅色/暗黑模式（当前为浅色模式）" aria-label="切换浅色/暗黑模式（当前为浅色模式）" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div><div class="searchHintContainer_Pkmr"><kbd class="searchHint_iIMx">ctrl</kbd><kbd class="searchHint_iIMx">K</kbd></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="最近博文导航"><div class="sidebarItemTitle_pO2u margin-bottom--md">All posts</div><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/paper/[00]intro">Intro</a></li><li class="sidebarItem__DBe"><a aria-current="page" class="sidebarItemLink_mo7H sidebarItemLinkActive_I1ZP" href="/paper/ImageNet Classification with Deep Convolutional Neural Network">AlexNet</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><article itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h1 class="title_f1Hy" itemprop="headline">AlexNet</h1><div class="container_mt6G margin-vert--md"><time datetime="2023-05-22T06:42:48.000Z" itemprop="datePublished">2023年5月22日</time> · <!-- -->阅读需 21 分钟</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="http://joyceliang.club/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.com/JoyceLiang-sudo.png" alt="Zhiying Liang"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="http://joyceliang.club/" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Zhiying Liang</span></a></div><small class="avatar__subtitle" itemprop="description">深度潜水选手</small></div></div></div></div></header><div id="post-content" class="markdown" itemprop="articleBody"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="imagenet-classification-with-deep-convolutional-neural-network">ImageNet Classification with Deep Convolutional Neural Network<a href="#imagenet-classification-with-deep-convolutional-neural-network" class="hash-link" aria-label="ImageNet Classification with Deep Convolutional Neural Network的直接链接" title="ImageNet Classification with Deep Convolutional Neural Network的直接链接">​</a></h2><p>Authors: Krizhevsky, Alex Sutskever, Ilya Hinton, Geoffrey E.</p><p>DOI: <a href="https://doi.org/10.1145/3065386" target="_blank" rel="noopener noreferrer">https://doi.org/10.1145/3065386</a></p><p>Year: 2017</p><p>期刊杂志: ILSVR2012 Champion, NIPS2012</p><p>Open Source: <a href="https://github.com/machrisaa/tensorflow-vgg" target="_blank" rel="noopener noreferrer">https://github.com/machrisaa/tensorflow-vgg</a> 非官方</p><p>Future: 解决video上的有监督学习问题</p><p>Meaning: 在无监督学习的大环境中，用大数据的有监督学习一举击败无监督，开辟彩虹大道</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="1-第一遍-标题-摘要-结论">1. 第一遍 标题 摘要 结论<a href="#1-第一遍-标题-摘要-结论" class="hash-link" aria-label="1. 第一遍 标题 摘要 结论的直接链接" title="1. 第一遍 标题 摘要 结论的直接链接">​</a></h2><p><a href="https://www.bilibili.com/video/BV1ih411J7Kz/?spm_id_from=333.788&amp;vd_source=b60872cd374332ccebd93d8a3907fbc7" target="_blank" rel="noopener noreferrer">沐神讲解</a></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="1⃣️-标题">1⃣️ 标题<a href="#1⃣️-标题" class="hash-link" aria-label="1⃣️ 标题的直接链接" title="1⃣️ 标题的直接链接">​</a></h3><p>ImageNet Classification with Deep Convolutional Neural Network</p><ul><li><code>ImageNet</code>：当时最大的图片分类数据集，100w图片 1000类别</li><li><code>Deep Convolutional</code>：<code>卷积神经网络</code>工作原理是什么? 同时作者为什么要使用 <code>深度</code>的卷积神经网络。2012 convolution 没有tree SVM🔥</li><li><code>Neural Networks</code> ：神经网络，这篇文章使用了神经网络技术。</li></ul><p><img loading="lazy" alt="img" src="/assets/images/img-d84907246de12e71966da854a8bc2cc8.png" width="1017" height="348" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="2⃣️-摘要">2⃣️ 摘要<a href="#2⃣️-摘要" class="hash-link" aria-label="2⃣️ 摘要的直接链接" title="2⃣️ 摘要的直接链接">​</a></h3><ul><li><p>干了什么？</p><p>训练了一个large and deep的CNN，来分类120w图片的1000个类别</p></li><li><p>效果如何？</p><p>比前人工作好</p><p>top-1 error: 37.5%</p><p>top-5 error: 17.0%</p></li><li><p>网络结构模样？</p><p>600w参数 65w神经元</p><p>5个卷积层（&lt;5 max-pooling层）+ 3个全连接层（1000-way softmax）</p></li><li><p>参数太多，提高训练速度？</p><p>non-saturating neurons + GPU实现卷积运算</p></li></ul><p><a href="https://www.zhihu.com/question/264163033" target="_blank" rel="noopener noreferrer">什么是non-saturating neurons非饱和神经元？</a></p><ul><li><p>参数太多，过拟合了怎么办？</p><p>避免FCN的过拟合，dropout正则effective</p></li><li><p>为什么我这么厉害？</p><p>不告诉你，反正我是ILSVRC-2012的🏆，错误率比🥈低了10.9%</p></li></ul><p><img loading="lazy" alt="img" src="/assets/images/img1-03590fcb9c88d256cdb0aa963ac3edc4.png" width="853" height="458" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="3⃣️-结论">3⃣️ 结论<a href="#3⃣️-结论" class="hash-link" aria-label="3⃣️ 结论的直接链接" title="3⃣️ 结论的直接链接">​</a></h3><p>无conclusion和Abstract一一对应，只有discussion(吐槽) and future</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="part1--文章总结">Part1  文章总结<a href="#part1--文章总结" class="hash-link" aria-label="Part1  文章总结的直接链接" title="Part1  文章总结的直接链接">​</a></h4><ul><li><p><strong>一句话，怎么总结我的好？</strong></p><p>a large, deep convolutional neural network is capable of achieving record-breaking results (SOTA) on a highly challenging dataset（指的是ImageNet）using purely supervised learning.</p></li><li><p><strong>什么情况，我会表现的不好呢？</strong></p><p>remove a single convolutional layer</p><p>i.e.,去掉中间层，降2%</p></li></ul><p><strong>Depth is important</strong></p><ul><li><p><strong>深度重要，但深度是最重要的吗？</strong></p><p>去掉一层convolutional layer, 降低2%，不能证明深度是最重要的</p></li><li><p><strong>可能的情况</strong>：没设置好参数</p><p>AlexNet可以去掉一些层，调节中间参数，效果不变。直接砍掉一层，掉2%可能是搜索参数做的不够，没调好参数</p></li><li><p><strong>反过来讲，结论没问题？</strong></p><p> <strong>深宽都重要</strong>，i.e.,照片的高宽比</p><p> 深度重要 → CNN需要<strong>很深</strong></p><p>宽度也重要 → 特别深 + 特别窄 or 特别浅 + 特别宽 ❌</p></li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="part2-未来研究">Part2 未来研究<a href="#part2-未来研究" class="hash-link" aria-label="Part2 未来研究的直接链接" title="Part2 未来研究的直接链接">​</a></h4><ul><li><p><strong>我们没有做什么？</strong></p><p>did not use any unsupervised pre-training</p></li><li><p><strong>不用unsupervised pre-training 也没关系？</strong></p><p>2012年的DL的目的是：像”人“（不一定知道真实答案） 书读百遍 其义自现</p><p>通过训练一个非常大的神经网络，在没有标签的数据上，把数据的内在结构抽出来</p></li><li><p><strong>关注的潮流怎么改变？</strong></p><p>AlexNet之前大佬们爱：无监督学习</p><p>（<strong>Why dalao们不喜欢 有监督学习？</strong>）</p><p>（因为有监督学习打不过 树 SVM 😊）</p><p>AlexNet 证明大力出奇迹，模型够大，有标签数据够多，我🏆</p><p>最近大家一起爱：BERT、GAN</p></li><li><p><strong>我们认为pre-training为什么好？</strong></p><p>有充足计算资源可以增加网络size时，无需增加标注数据</p></li><li><p><strong>我们有多牛？</strong></p><p>我们可以通过 让网络变大，训练更久，变得更强</p><p>但2012年的结果 和人类比还是有差距的</p><p>Note: 现在图片里找简单的物品，DL比人类好很多，图片识别在无人驾驶的应用</p></li><li><p><strong>我们怎么继续🐮呢？</strong></p><p>在video上训练very large and deep CNN, 因为video里的时序信息可以辅助理解图片中的空间信息</p></li><li><p><strong>这么牛的事情，大家做到了吗？</strong></p><p>目前，video还是很难。why? 图片和语言进展不错，video 相对于图片的计算量大幅增加，video的版权问题</p><p><img loading="lazy" alt="img" src="/assets/images/img2-04772b0792d762e6f2af224bf7d17e13.png" width="913" height="407" class="img_ev3q"></p></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="4⃣️-重要的图和公式">4⃣️ 重要的图和公式<a href="#4⃣️-重要的图和公式" class="hash-link" aria-label="4⃣️ 重要的图和公式的直接链接" title="4⃣️ 重要的图和公式的直接链接">​</a></h3><ul><li><p><strong>结果测试展示：</strong></p><p>效果在比较难的case表现不错</p></li></ul><p><img loading="lazy" alt="img" src="/assets/images/img3-6bd894701ca2b95f2ed81e7d30b07903.png" width="470" height="385" class="img_ev3q"></p><p> motor scooter、leopard雪豹、grille敞篷车 ✅</p><p> cherry ❌</p><p><img loading="lazy" alt="img" src="/assets/images/img4-6a00f85ef38c4aaca11cca2c56578009.png" width="532" height="385" class="img_ev3q"></p><p>  向量集合：输入图片在CNN的倒数第二层的数，作为每个图片的语义向量</p><p>给定一张图片，返回和我向量相似的图片；结果靠谱，🌹、🐘、🎃、🐶 都差不多</p><ul><li><p><strong>本文最重要的是什么？real wow moment</strong></p><p>Deep CNN训练的结果，图片最后向量（学到了一种嵌入表示）的语义表示特别好～！</p><p>相似的图片的向量会比较近，学到了一个非常好的特征；非常适合后面的ML，一个简单的softmax就能分类的很好！</p><p>学习嵌入表示，DL的一大强项</p><p>和当前最好的结果的对比：远远超过别人（卖点、wow moment、sexy point）</p><p><img loading="lazy" alt="img" src="/assets/images/img5-f3fc53753fc5c769295971f7763af865.png" width="425" height="235" class="img_ev3q"></p><p>96个卷积核，学习不同模式</p><p><img loading="lazy" alt="img" src="/assets/images/img6-9a225af0d0f840cf359bf8a4c42eae13.png" width="409" height="358" class="img_ev3q"></p><p>模型架构图</p><p><img loading="lazy" alt="img" src="/assets/images/img7-03322a8029dca1685b9555820fcd585d.png" width="1026" height="481" class="img_ev3q"></p><p>第一遍可能看不懂</p><p><strong>第一遍能看懂什么图？</strong></p><p>实验结果图，比较了解的方向的模型结构图。以后第一遍读论文，遇到比较新、开创性、看不懂的模型结构图，第一遍放下，后面再看</p><p><strong>第一遍的印象：</strong>结果特别好、NN实现的、为什么好？怎么做的？</p><p><strong>第一遍读完做什么？</strong></p><p>要不要继续读？</p><p>不读：很好用的 视觉网络；研究无关，放弃</p><p>读：CV研究者，工作很好，赢了今年的比赛，明年大家都用这个模型打比赛，我不试试吗？hhhh</p><p>参考：<a href="https://www.bilibili.com/read/cv13828723?from=note" target="_blank" rel="noopener noreferrer">沐神讲解</a></p></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="2-第二遍-过一遍看">2. 第二遍 过一遍看<a href="#2-第二遍-过一遍看" class="hash-link" aria-label="2. 第二遍 过一遍看的直接链接" title="2. 第二遍 过一遍看的直接链接">​</a></h2><p><a href="https://www.bilibili.com/video/BV1hq4y157t1/?spm_id_from=333.788&amp;vd_source=b60872cd374332ccebd93d8a3907fbc7" target="_blank" rel="noopener noreferrer">沐神讲解</a></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="1-introduction">1. Introduction<a href="#1-introduction" class="hash-link" aria-label="1. Introduction的直接链接" title="1. Introduction的直接链接">​</a></h3><p>1⃣️ 第一段</p><p><img loading="lazy" alt="img" src="/assets/images/img8-70701e004286fcd3b7342815a51448fb.png" width="914" height="323" class="img_ev3q"></p><p>2⃣️ 第二段</p><p>描述了怎么做神经网络，这里只介绍了CNN</p><p>写论文的时候 ，千万不要只说自己这个领域这个小方向大概怎么样，还要提到别的方向怎么样</p><p><img loading="lazy" alt="img" src="/assets/images/img9-f85d41761ce5dbd4d638258db86ebe7d.png" width="914" height="253" class="img_ev3q"></p><p>3⃣️ 第三段</p><p>4⃣️ 第四段</p><p><img loading="lazy" alt="img" src="/assets/images/img10-5ca7369bf381c49df447014e4a9f0207.png" width="909" height="299" class="img_ev3q"></p><p>作者还强调了由于GPU内存的限制，在两块GPU上进行训练时需要5-6天时间，如果能有更快的GPU和更大的数据集，网络性能还能进一步提升</p><p><img loading="lazy" alt="img" src="/assets/images/img11-5cf670504d3d94846086d1c692f75d2f.png" width="913" height="106" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="2-the-dataset">2. The Dataset<a href="#2-the-dataset" class="hash-link" aria-label="2. The Dataset的直接链接" title="2. The Dataset的直接链接">​</a></h3><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">介绍了整个数据集大约有1500万张图片，共有22000类。ILSVRC比赛共有1000类，每一类大约有1000张图片。在2010的比赛中，可以得到测试集数据标签，但是在2012年的比赛中则没有测试集标签</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>   由于ImageNet数据集图片精度并不相同，因此我们每一张图片下采样到256 × 256。当短边尺寸小于256时，我们先上采样到256，然后再从图片中截取 256 × 256的图片作为输入。我们没有对图片进行任何的预处理，整个网络是在每个像素的原始RGB值进行训练（也就是端到端训练，这也是深度学习的一大优势）</p><ul><li>将图片的短边减少到256，长边是保证高宽比不变的情况下也往下降，长边如果依然多出来的话，如果多于256的话，就以中心为界将两边裁掉，裁成一个256*256的图片</li><li>没有做任何的预处理，只是对图片进行了裁剪</li><li>网络是在raw RGB Value上训练的</li><li>当时做计算机视觉都是将特征抽出来，抽SIFT也好，抽别的特征也好（imagenet数据集也提供了一个SIFT版本的特征），这篇文章说不要抽特征，直接是在原始的Pixels上做了</li><li>在之后的工作里面基本上主要是end to end（端到端）：及那个原始的图片或者文本直接进去，不做任何的特征提取，神经网络能够帮助你完成这部分工作</li></ul><p><img loading="lazy" alt="img" src="/assets/images/img12-bc088346a0db5ee3234b5a1366d021d6.png" width="912" height="572" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="3-the-architecture">3. The Architecture<a href="#3-the-architecture" class="hash-link" aria-label="3. The Architecture的直接链接" title="3. The Architecture的直接链接">​</a></h3><ul><li>3.1 ReLU非线性激活函数，函数形式为 f(x) = max(0,x)，形式比较简单。作者说这是一个非饱和的非线性函数，比其它激活函数训练要快（具体原因没解释），关于ReLU函数的具体介绍可以看参考文献<!-- -->[20]<!-- -->。作者做了一个实验，如图1所示，在将错误率降低到25%时，ReLU比tanh函数训练时要快6倍</li></ul><p><img loading="lazy" alt="img" src="/assets/images/img13-a37131664f70d9b695ed4e6de1d3aa61.png" width="928" height="769" class="img_ev3q"></p><ul><li><p>3.2 使用了多GPU进行训练</p><p><img loading="lazy" alt="img" src="/assets/images/img14-1b544651ee2a057482e55aa4fed15287.png" width="904" height="658" class="img_ev3q"></p></li><li><p>3.3 正则化、归一化</p><p><img loading="lazy" alt="img" src="/assets/images/img15-fd016e27f00cad246d3664bb0780794d.png" width="923" height="615" class="img_ev3q"></p></li><li><p>3.4 Overlapping Pooling</p><p><img loading="lazy" alt="img" src="/assets/images/img16-0d8d714a5dceb883c3c1331263940dfb.png" width="907" height="297" class="img_ev3q"></p></li><li><p>3.5 Overall Architecture</p><p><img loading="lazy" alt="img" src="/assets/images/img17-7d13cd813f6073d3ee7e3e0ab6ba0e24.png" width="914" height="406" class="img_ev3q"></p><p><img loading="lazy" alt="img" src="/assets/images/img18-d6fbafb1a77a6b9451ec109f57e65923.png" width="908" height="179" class="img_ev3q"></p></li></ul><p><img loading="lazy" alt="img" src="/assets/images/img19-d4e61635d21018f869bbcd6925751a71.png" width="888" height="445" class="img_ev3q"></p><ul><li>方框表示每一层的输入和输出的数据的大小</li><li>输入的图片是一个高宽分别为224 x 224的3通道RGB图片</li><li>第一层卷积：卷积的窗口是11 x 11，有48个输出通道，stride等于4</li><li>有两个GPU，GPU1和GPU0都有自己的卷积核参数</li></ul><p><img loading="lazy" alt="img" src="/assets/images/img20-cb35837b4e49916e2ba8c8fe606935bb.png" width="562" height="352" class="img_ev3q"></p><ul><li>第1个卷积层在两个GPU上各有一个</li><li>第2个卷积层是在每个GPU把当前的卷积结果拿过来（GPU0的第二个卷积层读的是GPU0的第一个卷积层的卷积结果，GPU0和GPU1之间没有任何通讯）</li><li>到第3个卷积层的时候，GPU还是每个GPU上有自己的卷积核，但是每个卷积核会同时将第二个卷积层中GPU0和GPU1的卷积结果作为输入，两个GPU之间会通讯一次</li><li>第4、5个卷积层之间没有任何通讯</li><li>每个卷积层的通道数是不一样的，通道数有所增加，高和宽也有所变化</li><li>高宽慢慢变小、深度慢慢增加，随着深度的增加，慢慢地将空间信息压缩，直到最后每一个像素能够代表前面一大块的像素，然后再将通道数慢慢增加，可以认为每个通道数是去看一种特定的模式（例如192个通道可以简单地认为，能够识别图片中的192种不同的模式）</li><li>慢慢将空间信息压缩，语义空间慢慢增加，到最后卷积完之后，进入全连接层</li><li>全连接层中又出现了GPU之间的通讯，全连接层的输入是每个GPU第五个卷积的输出合并起来做全连接</li></ul><p><img loading="lazy" alt="img" src="/assets/images/img21-33df73fb8e16ce7622427a2a9937322b.png" width="594" height="350" class="img_ev3q"></p><ul><li>最后进入分类层的时候，变成了一个4096长的向量，每一块来自两个GPU，每片是2048，最后拼起来，所以一张图片会表示成一个4096维的向量，最后用一个线性分类做链接</li><li>深度学习的主要作用是将一张输入的图片，通过卷积、池化、全连接等一系列操作，将他压缩成一个长为4096的向量，这个向量能够将中间的语义信息都表示出来（将一个人能够看懂的像素通过一系列的特征提取变成了一个长为4096的机器能够看懂的东西，这个东西可以用来做搜索、分类等）</li><li>整个机器学习都可以认为是一个知识的压缩过程，不管是图片、语音还是文字或者视频，通过一个模型最后压缩成一个向量，然后机器去识别这个向量，然后在上面做各种事情</li><li>模型并行（model parallel）：现在在计算机视觉里面用的不多，但是在自然语言处理方面又成为主流了（将模型切开进行训练）</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="4-reducing-overfitting">4. Reducing Overfitting<a href="#4-reducing-overfitting" class="hash-link" aria-label="4. Reducing Overfitting的直接链接" title="4. Reducing Overfitting的直接链接">​</a></h3><p><strong>数据增强(data augmentation)</strong></p><ul><li>把一些图片人工地放大</li><li>在图片中随机地抠出一部分区域，做一些新的图片</li><li>把整个RGB的颜色通道channel上做一些改变，这里使用的是<strong>PCA(主成分分析)</strong>的方法，颜色会有不同，因此每次图片跟原始图片 是有一定的不同的</li></ul><p><img loading="lazy" alt="img" src="/assets/images/img22-ca52defcd5ced62134abe6480dbbee33.png" width="927" height="556" class="img_ev3q"></p><p><img loading="lazy" alt="img" src="/assets/images/img23-6c0da4b43eef314dd19e9a0956457f3c.png" width="920" height="283" class="img_ev3q"></p><p> <strong>Dropout</strong></p><ul><li>随机的把一些隐藏层的输出变成用50%的概率设为0，每一次都是把一些东西设置为0，所以模型也就发生了变化，每次得到一个新的模型，但是这些模型之间权重是共享的除了设置成0的，非0的东西都是一样的，这样就等价于做了模型融合</li><li>后来大家发现dropout其实也不是在做模型融合，更多的dropout就是一个正则项(dropout在现行模型上等价于一个L2正则项)</li><li>这里将dropout用在了前面的两个全连接层上面</li><li>文章说没有dropout的话，overfitting会非常严重，有dropout的话，训练会比别人慢两倍</li><li>现在CNN的设计通常不会使用那么大的全连接层，所以dropout也不那么重要，而且GPU、内存也没那么吃紧了</li><li>dropout在全连接层上还是很有用的，在RNN和Attension中使用的非常多</li></ul><p><img loading="lazy" alt="img" src="/assets/images/img24-9c26e0b46694557f0ac1b020f4a95036.png" width="910" height="439" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="5-details-of-learning">5. Details of learning<a href="#5-details-of-learning" class="hash-link" aria-label="5. Details of learning的直接链接" title="5. Details of learning的直接链接">​</a></h3><p><strong>讲述了模型是如何训练</strong></p><ul><li>使用SGD（随机梯度下降）来进行训练，SGD调参相对来说可能会比较难调，后来发现SGD里面的噪音对模型的泛化性其实是有好处的，所以现在深度学习中普遍使用SGD对模型进行训练。在这个文章之后SGD基本上在机器学习界成为了最主流的一个优化算法</li><li>batch size = 128</li><li>momentum = 0.9</li><li>weight decay是0.0005，也就是L2正则项，但是这个东西不是加在模型上，而是加在优化算法上，虽然他们两个是等价关系，但是因为深度学习的学习，所以大家现在基本上把这个东西叫做weight decay了</li><li>momentum也是因为这篇文章之后用的特别多，虽然在2010年的时候有大量的加速算法，里面有很fancy的各种加速SGD算法，但是现在看起来似乎用一个简单的momentum也是不错的</li><li>momentum实际上是，当优化的表面非常不平滑的时候，冲量使得不要被当下的梯度过多的误导，可以保持一个冲量从过去那个方向，沿着一个比较平缓的方向往前走，这样子不容易陷入到局部最优解</li><li>权重用的是一个均值为0，方差为0.01的高斯随机变量来初始化（0.01对很多网络都是可以的，但是如果特别深的时候需要更多优化，但是对于一些相对简单的神经网络，0.01是一个不错的选项）</li><li>现在就算是比较大的那些BERT，也就是用了0.02作为随机的初始值的方差</li><li>在第二层、第四层和第五层的卷积层把初始的偏移量初始化成1，剩下的全部初始化成0</li><li>每个层使用同样的学习率，从0.01开始，然后呢如果验证误差不往下降了，就手动的将他乘以0.1，就是降低十倍</li><li>ResNet中，每训练120轮，学习率每30轮就下降0.1另外一种主流的做法就是，前面可以做得更长一点，必须能够60轮或者是100轮，然后再在后面下降</li><li>在Alex之后的很多训练里面，都是做规则性地将学习率往下下降十倍，这是一个非常主流的做法，但是现在很少用了，现在使用更加平滑的曲线来降低学习率，比如果用一个cos的函数比较平缓地往下降。一开始的选择也很重要，如果选的太大可能会发生爆炸，如果太小又有可能训练不动，所以现在主流的做法是学习率从0开始再慢慢上升，慢慢下降</li><li>模型训练了90个epoch，然后每一遍用的是ImageNet完整的120万张图片，需要5-6天在两个GTX GPU上训练</li></ul><p><img loading="lazy" alt="img" src="/assets/images/img25-291b2e636b3f95c3216dd57721799334.png" width="908" height="709" class="img_ev3q"></p><p><img loading="lazy" alt="img" src="/assets/images/img26-d364cff7804f6259a12bb3ef4d1abf73.png" width="910" height="51" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="6-results">6. Results<a href="#6-results" class="hash-link" aria-label="6. Results的直接链接" title="6. Results的直接链接">​</a></h3><ul><li><p>下面是论文实验结果部分，可以看到在 <code>ILSVRC-2010/2012</code>数据上作者都取得了最低的错误率，同时作者也在2009年版本的 <code>ImageNet</code>全部数据上进行了训练，不过在 <code>ImageNet</code>全部数据集上进行训练的研究比较少</p><p><img loading="lazy" alt="img" src="/assets/images/img27-780a80e62f44243078d912b61feb4ecb.png" width="915" height="1071" class="img_ev3q"></p></li><li><p>作者在训练时也发现了一些有意思的现象，就是两个GPU，一个GPU上和卷积核和图像颜色是有关的，一个和图像颜色是无关的，这个还待解释。另一个是图4所示，当最后一个隐藏层（4096）神经元欧几里得空间距离相近是，两张图片基本上是同一类（深度学习的解释性也是一个很重要的研究方向）</p></li></ul><p><img loading="lazy" alt="img" src="/assets/images/img28-9a3d4bbcb8bda32cb55c99ff10a108b6.png" width="919" height="281" class="img_ev3q"></p><p><img loading="lazy" alt="img" src="/assets/images/img29-ee83bf98e5f166537e36de1a795dbf86.png" width="910" height="1034" class="img_ev3q"></p></div><footer class="row docusaurus-mt-lg blogPostFooterDetailsFull_mRVl"><div class="col"><b>标签：</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/paper/tags/deeplearning">Deeplearning</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/paper/tags/image-classification">Image Classification</a></li></ul></div><div class="col margin-top--sm"><a href="https://github.com/BrickBar1024/brickbar1024.github.io/tree/main/paper/[01]AlexNet.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>编辑此页</a></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="博文分页导航"><a class="pagination-nav__link pagination-nav__link--prev" href="/paper/[00]intro"><div class="pagination-nav__sublabel">较新一篇</div><div class="pagination-nav__label">Intro</div></a></nav></main><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#imagenet-classification-with-deep-convolutional-neural-network" class="table-of-contents__link toc-highlight">ImageNet Classification with Deep Convolutional Neural Network</a></li><li><a href="#1-第一遍-标题-摘要-结论" class="table-of-contents__link toc-highlight">1. 第一遍 标题 摘要 结论</a><ul><li><a href="#1⃣️-标题" class="table-of-contents__link toc-highlight">1⃣️ 标题</a></li><li><a href="#2⃣️-摘要" class="table-of-contents__link toc-highlight">2⃣️ 摘要</a></li><li><a href="#3⃣️-结论" class="table-of-contents__link toc-highlight">3⃣️ 结论</a></li><li><a href="#4⃣️-重要的图和公式" class="table-of-contents__link toc-highlight">4⃣️ 重要的图和公式</a></li></ul></li><li><a href="#2-第二遍-过一遍看" class="table-of-contents__link toc-highlight">2. 第二遍 过一遍看</a><ul><li><a href="#1-introduction" class="table-of-contents__link toc-highlight">1. Introduction</a></li><li><a href="#2-the-dataset" class="table-of-contents__link toc-highlight">2. The Dataset</a></li><li><a href="#3-the-architecture" class="table-of-contents__link toc-highlight">3. The Architecture</a></li><li><a href="#4-reducing-overfitting" class="table-of-contents__link toc-highlight">4. Reducing Overfitting</a></li><li><a href="#5-details-of-learning" class="table-of-contents__link toc-highlight">5. Details of learning</a></li><li><a href="#6-results" class="table-of-contents__link toc-highlight">6. Results</a></li></ul></li></ul></div></div></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2023 BrickBar, Inc. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.e24bfab6.js"></script>
<script src="/assets/js/main.f55d07d9.js"></script>
</body>
</html>