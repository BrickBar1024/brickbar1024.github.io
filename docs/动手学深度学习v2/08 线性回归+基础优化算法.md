# 08 线性回归+基础优化算法

# 线性回归

## 总结

<img src="http://img.peterli.club/joy/image-20230312212104345.png" alt="image-20230312212104345"/>

## 举例

![image-20230312223205380](http://img.peterli.club/joy/image-20230312223205380.png)

![image-20230312223224382](http://img.peterli.club/joy/image-20230312223224382.png)

## 线性模型

![image-20230312223236279](http://img.peterli.club/joy/image-20230312223236279.png)

![image-20230312223247106](http://img.peterli.club/joy/image-20230312223247106.png)

![image-20230312223258880](http://img.peterli.club/joy/image-20230312223258880.png)

## 衡量预估质量

![image-20230312223309113](http://img.peterli.club/joy/image-20230312223309113.png)

## 训练数据

![image-20230312223321024](http://img.peterli.club/joy/image-20230312223321024.png)

## 参数学习

![image-20230312223332341](http://img.peterli.club/joy/image-20230312223332341.png)

## 显示解

![image-20230312223342854](http://img.peterli.club/joy/image-20230312223342854.png)

> 纠：$W^* =$ ${(X^TX)}^{-1}X^Ty$
> 

# 基础优化方法

## 梯度下降

### Overview

> 一个模型没有显示解的时候该怎么办呢？
> 

先挑选一个参数的随机初始值，记为$W_0$，在后面不断更新$W_0$的值，使得它接近我们的最优解

![image-20230312223354211](http://img.peterli.club/joy/image-20230312223354211.png)

> $W_{t-1}$:上个时刻的参数值  $\eta$:学习率(标量  ♦️:损失函数$l$对$W_{t-1}$的梯度
> 

直观：

1⃣️ 外圈值最大，同一圈上的值一样

2⃣️ $W_0$:随机取的值

3⃣️ 梯度负方向(♦️：函数值下降最快的方向

4⃣️ $\eta$：沿着这个方向每一次走多远

### 选择学习率(hyperparameter

![image-20230312223403456](http://img.peterli.club/joy/image-20230312223403456.png)

### 小批量随机梯度下降

![image-20230312223411363](http://img.peterli.club/joy/image-20230312223411363.png)

> 用随机的b个样本近似所有样本的损失，b值大的话，近似精确，b值小的话，近似不太精确，但是梯度比较好算，梯度计算的复杂度跟样本的个数是线性相关的
> 

![image-20230312223419401](http://img.peterli.club/joy/image-20230312223419401.png)

### 总结

- 梯度下降通过不断沿着反梯度方向更新参数求解
- 小批量随机梯度下降是深度学习默认的求解算法(虽然有更好的算法，一般来说是最稳定和最简单的
- 两个重要的超参数是批量大小batch size和学习率learning rate

# 线性回归的从零开始实现

> 我们将从零开始实现整个方法，包括数据流水线、模型、损失函数和小批量随机梯度下降优化器
> 

```python
%matplotlib inline # plot的时候默认嵌入到notebook里面
import random
import torch
from d2l import torch as d2l
```

> 根据带有噪声的线性模型构造一个人造数据集。我们使用线性模型参数$w={[2,-3.4]}^T、b=4.2$和噪声项c生成数据集及其标签：$y=Xw+b+c$
> 

```python
def synthetic_data(w, b, num_examples):  
    """生成y=Xw+b+噪声"""
		# 均值为0，方差为1的随机数，num_examples样本个数，len(w)列的个数
    X = torch.normal(0, 1, (num_examples, len(w))
    y = torch.matmul(X, w) + b
		# 均值为0，方差为1的随机噪音，形状跟y一样
    y += torch.normal(0, 0.01, y.shape)
		# X,y作为列向量返回
    return X, y.reshape((-1, 1))
		
true_w = torch.tensor([2, -3.4])
true_b = 4.2
features, labels = synthetic_data(true_w, true_b, 1000)
```

> features 中的每一行都包含一个二维数据样本，labels中的每一行都包含一维标签值（一个标量)
> 

```python
print('features:', features[0],'\nlabel:', labels[0])
```

- Output
  
    features: tensor([-0.3679, -1.8471])
    label: tensor([9.7361])
    

```python
d2l.set_figsize()
d2l.plt.scatter(features[:, 1].detach().numpy(), labels.detach().numpy(), 1);
```

- Output
  
    ![image-20230312223433833](http://img.peterli.club/joy/image-20230312223433833.png)
    

> 定义一个data_iter函数，该函数接收批量大小、特征矩阵和标签向量作为输入，生成大小为batch_size的小批量
> 

```python
def data_iter(batch_size, features, labels):
    # 取的第一维长度
		num_examples = len(features)
    indices = list(range(num_examples))
		# 这些样本是随机读取的，没有特定的顺序
    random.shuffle(indices)
    for i in range(0, num_examples, batch_size):
        batch_indices = torch.tensor(
            indices[i: min(i + batch_size, num_examples)])
        yield features[batch_indices], labels[batch_indices]

batch_size = 10

for X, y in data_iter(batch_size, features, labels):
    print(X, '\n', y)
    break
```

- Output
  
    ![image-20230312223444411](http://img.peterli.club/joy/image-20230312223444411.png)
    

> 定义初始化模型参数
> 

```python
w = torch.normal(0, 0.01, size=(2,1), requires_grad=True)
b = torch.zeros(1, requires_grad=True)
```

> 定义模型
> 

```python
def linreg(X, w, b):  
    """线性回归模型"""
    return torch.matmul(X, w) + b
```

> 定义损失函数
> 

```python
def squared_loss(y_hat, y):  
    """均方损失"""
    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2
```

> 定义优化算法
> 

```python
def sgd(params, lr, batch_size):  
    """小批量随机梯度下降"""
		# 不需要计算梯度
    with torch.no_grad():
        for param in params:
            param -= lr * param.grad / batch_size
            param.grad.zero_()
```

> 训练过程
> 

```python
lr = 0.03
num_epochs = 3
net = linreg
loss = squared_loss

for epoch in range(num_epochs):
    for X, y in data_iter(batch_size, features, labels):
        l = loss(net(X, w, b), y) # ‘X’和'y'的小批量损失
				# 因为'l'形状是('batch_size', 1),而不是一个标量
				# ‘l’中的所有元素被加到一起，并以此计算关于['w','b']的梯度
        l.sum().backward()
				# 使用参数的梯度更新参数
        sgd([w, b], lr, batch_size)
    with torch.no_grad():
        train_l = loss(net(features, w, b), labels)
        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')
```

比较真实参数和通过训练学到的参数来评估训练的成功程度

```python
print(f'w的估计误差: {true_w - w.reshape(true_w.shape)}')
print(f'b的估计误差: {true_b - b}')
```

- Output
  
    w的估计误差: tensor([ 0.0003, -0.0002])
    b的估计误差: tensor([0.0010])

# 线性回归的简洁实现

通过使用深度学习框架来简洁地实现线性回归模型生成数据集

```python
import numpy as np
import torch
from torch.utils import data
from d2l import torch as d2l

true_w = torch.tensor([2, -3.4])
true_b = 4.2
features, labels = d2l.synthetic_data(true_w, true_b, 1000)
```

调用框架中现有的API来读取数据

```python
def load_array(data_arrays, batch_size, is_train=True):  
    """构造一个PyTorch数据迭代器"""
    dataset = data.TensorDataset(*data_arrays)
    return data.DataLoader(dataset, batch_size, shuffle=is_train)

batch_size = 10
data_iter = load_array((features, labels), batch_size)

next(iter(data_iter))
```

- Output
  
    ![image-20230312223520005](http://img.peterli.club/joy/image-20230312223520005.png)
    

使用框架的预定义好的层

```python
# 'nn'是神经网络的缩写
from torch import nn
# Sequential: list of layers 2:输入维度 1:输出维度 
net = nn.Sequential(nn.Linear(2, 1))
```

初始化模型参数

```python
net[0].weight.data.normal_(0, 0.01)
net[0].bias.data.fill_(0)
```

- Output
  
    tensor([0.])
    

计算均方误差使用的是MSELoss类，也称为平方$L_2$范数

```python
loss = nn.MSELoss()
```

实例化SGD实例

```python
trainer = torch.optim.SGD(net.parameters(), *lr*=0.03)
```

训练过程代码与我们从零开始实现时所做的非常相似

```python
num_epochs = 3
for epoch in range(num_epochs):
    for X, y in data_iter:
        l = loss(net(X) ,y)
				# 梯度清零
        trainer.zero_grad()
        l.backward()
				# 模型更新
        trainer.step()
    l = loss(net(features), labels)
    print(f'epoch {epoch + 1}, loss {l:f}')
```